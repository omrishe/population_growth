{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62ceb681",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47404c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "    import pandas as pd\n",
    "    import requests\n",
    "    import numpy as np\n",
    "    import time\n",
    "    import bs4\n",
    "    from bs4 import BeautifulSoup\n",
    "    import selenium\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.common.keys import Keys\n",
    "    from selenium.webdriver.common.by import By\n",
    "    from selenium.webdriver.chrome.service import Service\n",
    "    from webdriver_manager.chrome import ChromeDriverManager\n",
    "    from selenium.webdriver.support.select import Select\n",
    "    from selenium.webdriver.common.action_chains import ActionChains\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e27ee4",
   "metadata": {},
   "source": [
    "# population page extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bb9804",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_population(page,extract_page_catagory_name):\n",
    "     # and returns it\n",
    "    time.sleep(5)\n",
    "    anti_exception=True\n",
    "    error_counter=0\n",
    "    while anti_exception==True and error_counter<5: #anti program failsafe \n",
    "        try:\n",
    "            temp = page.find_element(By.XPATH, \"/html/body/form/div[3]/div/div/div/div[4]/table/tbody/tr[2]\")\n",
    "            anti_exception=False #will pass the temp and not get caught when \n",
    "        except:\n",
    "            print(\"failed to retrieve page element error number:\", error_counter)\n",
    "            error_counter=error_counter+1\n",
    "            \n",
    "            \n",
    "    col_names = temp.find_elements(By.TAG_NAME, \"th\")\n",
    "    col_names_list = [\"סמל יישוב\",\"שם יישוב\"]\n",
    "    for col_name in col_names:  # get columns names\n",
    "        col_names_list.append(col_name.text)\n",
    "    col_names_list[6]=col_names_list[6] + \"זכר\" #columns are seperatred between female and male\n",
    "    col_names_list[7]=col_names_list[7] + \"זכר\" #its location on the page is mixed with some columns names\n",
    "    col_names_list[8]=col_names_list[8] + \"זכר\" #which made it problemetic to extract \n",
    "    col_names_list[9]=col_names_list[9] + \"נקבה\"\n",
    "    col_names_list[10]=col_names_list[10] + \"נקבה\"\n",
    "    col_names_list[11]=col_names_list[11] + \"נקבה\"\n",
    "    df = pd.DataFrame(columns=col_names_list)\n",
    "    table = page.find_element(By.XPATH, \"/html/body/form/div[3]/div/div/div/div[4]/table/tbody\")  # get table\n",
    "    rows = table.find_elements(By.TAG_NAME, \"tr\")  # extract each row from a table\n",
    "    table_len = len(rows)  #\n",
    "    for row in rows[2:]:\n",
    "        elements = row.find_elements(By.TAG_NAME, \"td\")  # extract each element from a row\n",
    "        elements_text = []\n",
    "        for i in elements:  # go through each cell in a row (as element not text)\n",
    "            elements_text.append(i.text)  # convert element which is 1 cell in a row to text\n",
    "        df.loc[\n",
    "            len(df)] = elements_text  # adds a new line to the end of data frame len(df)return the length of the dataframe\n",
    "    return df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1a865f",
   "metadata": {},
   "source": [
    "# construction page extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb621ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_page_construction(page, extract_page_catagory_name):\n",
    "    time.sleep(3)\n",
    "    temp = page.find_element(By.XPATH, \"/html/body/form/div[3]/div/div/div/div[4]/table/tbody/tr[2]\")\n",
    "    col_names = temp.find_elements(By.TAG_NAME, \"th\")\n",
    "    col_names_list = []\n",
    "    for col_name in col_names:  # get columns names\n",
    "        col_names_list.append(col_name.text)\n",
    "    df = pd.DataFrame(columns=col_names_list)\n",
    "    table = page.find_element(By.XPATH, \"/html/body/form/div[3]/div/div/div/div[4]/table/tbody\")  # get table\n",
    "    rows = table.find_elements(By.TAG_NAME, \"tr\")  # extract each row from a table\n",
    "    for row in rows[2:]:\n",
    "        elements = row.find_elements(By.TAG_NAME, \"td\")  # extract each element from a row\n",
    "        elements_text = []\n",
    "        for i in elements:  # go through each cell in a row (as element not text)\n",
    "            elements_text.append(i.text)  # convert element which is 1 cell in a row to text\n",
    "        df.loc[\n",
    "            len(df)] = elements_text  # adds a new line to the end of data frame len(df)return the length of the dataframe\n",
    "    return df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfefdceb",
   "metadata": {},
   "source": [
    "# general page extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac14510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_page(page, extract_page_catagory_name):  # creates new dataframe (including columns names) from the page\n",
    "    # and returns it\n",
    "    print(\"extract_page:\", extract_page_catagory_name)\n",
    "    if(extract_page_catagory_name==\"construction\"):\n",
    "        return extract_page_construction(page, extract_page_catagory_name)\n",
    "    if extract_page_catagory_name in [\"information\", \"transportation\", \"construction\", \"social_economic index\"]:  # columns location differ between catagories\n",
    "        time.sleep(3)\n",
    "        temp = page.find_element(By.XPATH, \"/html/body/form/div[3]/div/div/div/div[4]/table/thead/tr\")\n",
    "        col_names = temp.find_elements(By.TAG_NAME, \"th\")\n",
    "    if(extract_page_catagory_name==\"population\"):\n",
    "        print(\"in population\")\n",
    "        return extract_population(page,extract_page_catagory_name)\n",
    "    col_names_list = []\n",
    "    for col_name in col_names:  # get columns names\n",
    "        col_names_list.append(col_name.text)\n",
    "    df = pd.DataFrame(columns=col_names_list)\n",
    "    table = page.find_element(By.XPATH, \"/html/body/form/div[3]/div/div/div/div[4]/table/tbody\")  # get table\n",
    "    rows = table.find_elements(By.TAG_NAME, \"tr\")  # extract each row from a table\n",
    "    table_len = len(rows)\n",
    "    for row in rows:\n",
    "        elements = row.find_elements(By.TAG_NAME, \"td\")  # extract each element from a row\n",
    "        elements_text = []\n",
    "        for i in elements:  # go through each cell in a row (as element not text)\n",
    "            elements_text.append(i.text)  # convert element which is 1 cell in a row to text\n",
    "        print(elements_text)\n",
    "        df.loc[\n",
    "            len(df)] = elements_text  # adds a new line to the end of data frame len(df)return the length of the dataframe\n",
    "    \n",
    "    return df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0b4ee6",
   "metadata": {},
   "source": [
    "# sub function to control past year population pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b295ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_population_by_year(driver,year):#name pending\n",
    "    time.sleep(3)\n",
    "    #from here we extract population\n",
    "    for i in range(3):#move catagories untill population catagory is reached\n",
    "        time.sleep(2)\n",
    "        catagory_menu = driver.find_element(By.XPATH, \"/html/body/form/div[3]/div/div/div/div[2]/span[2]/div\")\n",
    "        catagory_menu.click()\n",
    "        actions = ActionChains(driver)\n",
    "        actions.send_keys(Keys.ARROW_DOWN)\n",
    "        actions.send_keys(Keys.ENTER)\n",
    "        actions.perform()\n",
    "        \n",
    "    for index in range(2021-year):#change the year to 2018\n",
    "        time.sleep(1)\n",
    "        year_menu = driver.find_element(By.XPATH, \"/html/body/form/div[3]/div/div/div/div[2]/span[4]/div\")\n",
    "        year_menu.click()\n",
    "        actions.send_keys(Keys.ARROW_DOWN)\n",
    "        actions.send_keys(Keys.ENTER)\n",
    "        actions.perform()\n",
    "    pop=get_catagory(driver, 1, \"population\")# extract dataframe and send back\n",
    "    \n",
    "    \n",
    "    return pop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9065a32",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b89ca21",
   "metadata": {},
   "source": [
    "# catagory retrievel function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde41435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_catagory(pages, page_number, catagory_name=\"None\"): # default values\n",
    "    print(catagory_name)\n",
    "    catagory_dataframe = pd.DataFrame()\n",
    "    time.sleep(3)\n",
    "    click_array = driver.find_element(By.XPATH, \"/html/body/form/div[3]/div/div/div/div[5]/pager/div\")  # contains\n",
    "    # the pages and some more clickable\n",
    "    pages = click_array.find_elements(By.TAG_NAME, \"li\")#collect all clickable elements that are pages in 1 list\n",
    "    if page_number == 1:  # starting condition\n",
    "        catagory_dataframe = extract_page(driver, catagory_name).copy()\n",
    "        page_number = page_number + 1 \n",
    "    for page in pages:#go through the element list that contains all elements that are pages\n",
    "        if page.text == str(page_number):\n",
    "            print(\"scanning page:\", page_number)\n",
    "            page.click() #click the page so its loads in selenium\n",
    "            catagory_dataframe = pd.concat([catagory_dataframe, extract_page(driver, catagory_name)],\n",
    "                                           ignore_index=True).copy() #sends the page that is loaded to other function to extract the info from it and link it to the previous dataframe\n",
    "            page_number = page_number + 1\n",
    "        if(page.get_attribute(\"class\")==\"nextPages\"): #this one works removed for since second method is easier on the eye\\n\",\n",
    "                page.click()\n",
    "                catagory_dataframe=pd.concat([catagory_dataframe, get_catagory(driver,page_number,catagory_name)]) \n",
    "    return catagory_dataframe.copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2189a8cc",
   "metadata": {},
   "source": [
    "# main function 2017 catagory extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af3ab67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url = 'https://boardsgenerator.cbs.gov.il/pages/webparts/yishuvimpage.aspx'\n",
    "\n",
    "PATH = 'C:\\Program Files (x86)\\chromedriver.exe'\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(url)\n",
    "time.sleep(5)\n",
    "dataframe_catagory_list = []\n",
    "target_year = 2018\n",
    "catagory_names = [\"information\", \"transportation\", \"construction\", \"population\", \"health\", \"mifkad2008\",\n",
    "                  \"data_from_local_authorities\", \"salary\", \"welfare\", \"education\", \"water\", \"waste\",\n",
    "                  \"social_economic index\"]#the catagories in the page\n",
    "\n",
    "i = 0\n",
    "for category in catagory_names:#goes through all catagories in the list in order\n",
    "    anti_exception=True\n",
    "    error_counter=0\n",
    "    print(category)\n",
    "    if(category not in [\"information\", \"transportation\", \"construction\", \"population\",\"social_economic index\"]):#the catagory is not what we wanted\n",
    "        catagory_menu = driver.find_element(By.XPATH, \"/html/body/form/div[3]/div/div/div/div[2]/span[2]/div\")\n",
    "        catagory_menu.click()\n",
    "        actions = ActionChains(driver)\n",
    "        actions.send_keys(Keys.ARROW_DOWN)\n",
    "        actions.send_keys(Keys.ENTER)\n",
    "        actions.perform()\n",
    "        continue\n",
    "    time.sleep(3)\n",
    "    print(\"Scanning: \", category)\n",
    "    actions = ActionChains(driver)\n",
    "    if(category !=\"construction\"):\n",
    "        while anti_exception==True and error_counter<5: #anti crash failsafe \n",
    "            try:\n",
    "                temp = driver.find_element(By.XPATH, \"/html/body/form/div[3]/div/div/div/div[4]/table/tbody/tr[2]\")\n",
    "                year_menu = driver.find_element(By.XPATH, \"/html/body/form/div[3]/div/div/div/div[2]/span[4]/div\")\n",
    "                anti_exception=False #if failed loading the element it wont reach here,it will reach only when no error was found loading the element\n",
    "            except:\n",
    "                print(\"failed to retrieve year menu element error number:\", error_counter)\n",
    "                error_counter=error_counter+1\n",
    "    if category == \"social_economic index\":#change the year to 2017\n",
    "        year_menu.click()\n",
    "        actions.send_keys(Keys.ARROW_DOWN)\n",
    "        actions.send_keys(Keys.ENTER)\n",
    "        actions.perform()\n",
    "        time.sleep(3)\n",
    "    elif category == \"salary\":#change the year to 2017\n",
    "        for index in range(2):\n",
    "            year_menu.click()\n",
    "            actions.send_keys(Keys.ARROW_DOWN)\n",
    "            actions.send_keys(Keys.ENTER)\n",
    "            actions.perform()\n",
    "            time.sleep(3)\n",
    "    if category == \"information\" or category == \"transportation\" or category == \"population\" :\n",
    "        for index in range(2022 - target_year):#change the year to 2017\n",
    "            year_menu.click()\n",
    "            actions.send_keys(Keys.ARROW_DOWN)\n",
    "            actions.send_keys(Keys.ENTER)\n",
    "            actions.perform()\n",
    "            time.sleep(3)\n",
    "    dataframe_catagory_list.append(\n",
    "        get_catagory(driver, 53, category))  # extract the all the pages in the catagory and send the catagory name \n",
    "    #and add to a list of dataframes\n",
    "    #path = \"C:\\\\Users\\\\omri\\\\Desktop\\\\\" #C:\\Users\\ONE1\\Desktop\\omri\n",
    "    try:#instead of changing each time,try to save to all 3 known locations\n",
    "        dataframe_catagory_list[i].to_csv(\"C:\\\\Users\\\\omri\\\\Desktop\\\\\" + category + \".csv\", index=False,\n",
    "                                      encoding='utf-8-sig') #my main pc\n",
    "    except:\n",
    "        try:\n",
    "            dataframe_catagory_list[i].to_csv(\"C:\\\\Users\\\\ONE1\\\\Desktop\\\\omri\\\\\" + category + \".csv\", index=False,\n",
    "                                      encoding='utf-8-sig')#my laptop\n",
    "        except:\n",
    "            try:\n",
    "                dataframe_catagory_list[i].to_csv(\"C:\\\\Users\\\\ONE1\\\\Desktop\\\\omri\\\\\" + category + \".csv\", index=False,\n",
    "                                      encoding='utf-8-sig') # nir change this to where you want to save\n",
    "            except:\n",
    "                raise # raise error if cant save\n",
    "          \n",
    "        #delete below this is a test\n",
    "    catagory_menu = driver.find_element(By.XPATH, \"/html/body/form/div[3]/div/div/div/div[2]/span[2]/div\")\n",
    "    catagory_menu.click()\n",
    "    actions = ActionChains(driver)\n",
    "    actions.send_keys(Keys.ARROW_DOWN)\n",
    "    actions.send_keys(Keys.ENTER)\n",
    "    actions.perform()\n",
    "    i = i + 1\n",
    "driver.quit()\n",
    "time.sleep(5)\n",
    "modular_population_extractor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642fc048",
   "metadata": {},
   "source": [
    "# population 2013-2016 extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b0999a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modular_population_extractor():\n",
    "    url = 'https://boardsgenerator.cbs.gov.il/pages/webparts/yishuvimpage.aspx'\n",
    "    path = 'C:\\Program Files (x86)\\chromedriver.exe'\n",
    "    year_array=[2018,2017,2016,2015,2014,2013] #add year \n",
    "    file_path='C:\\\\Users\\\\ONE1\\\\Desktop\\\\omri\\\\'  #C:\\Users\\ONE1\\Desktop\\omri\n",
    "    for pop_year in year_array:\n",
    "        print(\"loading year:\",pop_year)\n",
    "        time.sleep(3)\n",
    "        driver = webdriver.Chrome()\n",
    "        driver.get(url)\n",
    "        pop_dataframe=extract_population_by_year(driver,pop_year)\n",
    "        pop_dataframe.to_csv(file_path + \"pop\" + str(pop_year) + \".csv\", index=False,encoding='utf-8-sig')\n",
    "    driver.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
